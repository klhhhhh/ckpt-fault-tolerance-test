Master Port: 63568
srun: warning: can't run 1 processes on 16 nodes, setting nnodes to 1
Master Address: 128.55.69.129
Master Port: 63568
srun: Step created for StepId=35554199.1
nid002836
nid002904
nid002913
nid003089
nid002976
nid002956
nid002981
nid002964
nid003060
nid003072
nid002892
nid003100
nid002900
nid003056
nid003000
nid002920
W0207 17:31:29.430000 139973080128384 torch/distributed/run.py:779] 
W0207 17:31:29.430000 139973080128384 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.430000 139973080128384 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.430000 139973080128384 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.430000 140576330570624 torch/distributed/run.py:779] 
W0207 17:31:29.430000 140576330570624 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.430000 140576330570624 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.430000 140576330570624 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.430000 139802892917632 torch/distributed/run.py:779] 
W0207 17:31:29.430000 139802892917632 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.430000 139802892917632 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.430000 139802892917632 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.431000 139972049111936 torch/distributed/run.py:779] 
W0207 17:31:29.431000 139972049111936 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.431000 139972049111936 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.431000 139972049111936 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.438000 140522676501376 torch/distributed/run.py:779] 
W0207 17:31:29.438000 140522676501376 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.438000 140522676501376 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.438000 140522676501376 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.453000 140569550056320 torch/distributed/run.py:779] 
W0207 17:31:29.453000 140569550056320 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.453000 140569550056320 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.453000 140569550056320 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.465000 140072797277056 torch/distributed/run.py:779] 
W0207 17:31:29.465000 140072797277056 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.465000 140072797277056 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.465000 140072797277056 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.471000 140133753011072 torch/distributed/run.py:779] 
W0207 17:31:29.471000 140133753011072 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.471000 140133753011072 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.471000 140133753011072 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.471000 140552122837888 torch/distributed/run.py:779] 
W0207 17:31:29.471000 140552122837888 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.471000 140552122837888 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.471000 140552122837888 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.481000 140142665034624 torch/distributed/run.py:779] 
W0207 17:31:29.481000 140142665034624 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.481000 140142665034624 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.481000 140142665034624 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.487000 140350107757440 torch/distributed/run.py:779] 
W0207 17:31:29.487000 140350107757440 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.487000 140350107757440 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.487000 140350107757440 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.534000 140224854911872 torch/distributed/run.py:779] 
W0207 17:31:29.534000 140224854911872 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.534000 140224854911872 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.534000 140224854911872 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.537000 140005768510336 torch/distributed/run.py:779] 
W0207 17:31:29.537000 140005768510336 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.537000 140005768510336 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.537000 140005768510336 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.562000 140447366531968 torch/distributed/run.py:779] 
W0207 17:31:29.562000 140447366531968 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.562000 140447366531968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.562000 140447366531968 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.683000 140111894322048 torch/distributed/run.py:779] 
W0207 17:31:29.683000 140111894322048 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.683000 140111894322048 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.683000 140111894322048 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.971000 140133796002688 torch/distributed/run.py:779] 
W0207 17:31:29.971000 140133796002688 torch/distributed/run.py:779] *****************************************
W0207 17:31:29.971000 140133796002688 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0207 17:31:29.971000 140133796002688 torch/distributed/run.py:779] *****************************************
[NeMo W 2025-02-07 17:31:57 nemo_logging:349] /pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, input, weight, bias, allreduce_dgrad):
    
[NeMo W 2025-02-07 17:31:57 nemo_logging:349] /pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2025-02-07 17:31:57 nemo_logging:349] /pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2025-02-07 17:31:57 nemo_logging:349] /pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2025-02-07 17:32:00 nemo_logging:349] /pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor
    
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Ambiguous value for argument 'model.data.splits_string=980,10,10'
1. To use it as a list, use key=[value1,value2]
2. To use it as string, quote the value: key=\'value1,value2\'
3. To sweep over it, add --multirun to your command line

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
W0207 17:32:12.363000 140224854911872 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 606507 closing signal SIGTERM
W0207 17:32:12.369000 140552122837888 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1687539 closing signal SIGTERM
E0207 17:32:12.377000 140552122837888 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1687537) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
E0207 17:32:12.379000 140224854911872 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 606506) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
E0207 17:32:12.380000 139973080128384 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1734267) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
E0207 17:32:12.384000 140111894322048 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 303073) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
W0207 17:32:12.389000 140133796002688 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1396963 closing signal SIGTERM
E0207 17:32:12.390000 140133796002688 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1396962) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
W0207 17:32:12.399000 140522676501376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 482837 closing signal SIGTERM
W0207 17:32:12.399000 140522676501376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 482838 closing signal SIGTERM
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
W0207 17:32:12.399000 140522676501376 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 482840 closing signal SIGTERM
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    raise ChildFailedError(
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    return launch_agent(self._config, self._entrypoint, list(args))
    run(args)
    return f(*args, **kwargs)
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002981-hsn0
  rank      : 37 (local_rank: 1)
  exitcode  : 1 (pid: 1687538)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002981-hsn0
  rank      : 39 (local_rank: 3)
  exitcode  : 1 (pid: 1687540)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002981-hsn0
  rank      : 36 (local_rank: 0)
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  exitcode  : 1 (pid: 1687537)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    return launch_agent(self._config, self._entrypoint, list(args))
    raise ChildFailedError(
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
    elastic_launch(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002920-hsn0
  rank      : 21 (local_rank: 1)
  exitcode  : 1 (pid: 303074)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002920-hsn0
  rank      : 22 (local_rank: 2)
  exitcode  : 1 (pid: 303075)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid002920-hsn0
  rank      : 23 (local_rank: 3)
  exitcode  : 1 (pid: 303076)
  error_file: <N/A>
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    raise ChildFailedError(
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002920-hsn0
  rank      : 20 (local_rank: 0)
  exitcode  : 1 (pid: 303073)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002892-hsn0
  rank      : 6 (local_rank: 2)
  exitcode  : 1 (pid: 606508)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002892-hsn0
  rank      : 7 (local_rank: 3)
  exitcode  : 1 (pid: 606509)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002892-hsn0
  rank      : 4 (local_rank: 0)
  exitcode  : 1 (pid: 606506)
    raise ChildFailedError(
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid003100-hsn0
  rank      : 62 (local_rank: 2)
  exitcode  : 1 (pid: 1396964)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid003100-hsn0
  rank      : 63 (local_rank: 3)
  exitcode  : 1 (pid: 1396965)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid003100-hsn0
  rank      : 60 (local_rank: 0)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
  exitcode  : 1 (pid: 1396962)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid003072-hsn0
  rank      : 53 (local_rank: 1)
  exitcode  : 1 (pid: 1734268)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid003072-hsn0
  rank      : 54 (local_rank: 2)
  exitcode  : 1 (pid: 1734269)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid003072-hsn0
  rank      : 55 (local_rank: 3)
  exitcode  : 1 (pid: 1734270)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid003072-hsn0
  rank      : 52 (local_rank: 0)
  exitcode  : 1 (pid: 1734267)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
E0207 17:32:12.407000 140447366531968 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 242360) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
============================================================
E0207 17:32:12.409000 140142665034624 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2211713) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
E0207 17:32:12.417000 140576330570624 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1329649) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    return f(*args, **kwargs)
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    return launch_agent(self._config, self._entrypoint, list(args))
E0207 17:32:12.438000 140522676501376 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 482839) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid003060-hsn0
  rank      : 49 (local_rank: 1)
  exitcode  : 1 (pid: 2211714)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid003060-hsn0
  rank      : 50 (local_rank: 2)
  exitcode  : 1 (pid: 2211715)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid003060-hsn0
  rank      : 51 (local_rank: 3)
  exitcode  : 1 (pid: 2211716)
  error_file: <N/A>
    run(args)
    raise ChildFailedError(
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid003060-hsn0
  rank      : 48 (local_rank: 0)
  exitcode  : 1 (pid: 2211713)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002904-hsn0
  rank      : 13 (local_rank: 1)
  exitcode  : 1 (pid: 242361)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002904-hsn0
  rank      : 14 (local_rank: 2)
  exitcode  : 1 (pid: 242362)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid002904-hsn0
  rank      : 15 (local_rank: 3)
  exitcode  : 1 (pid: 242363)
  error_file: <N/A>
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002904-hsn0
  rank      : 12 (local_rank: 0)
  exitcode  : 1 (pid: 242360)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
    return launch_agent(self._config, self._entrypoint, list(args))
============================================================
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002913-hsn0
  rank      : 17 (local_rank: 1)
  exitcode  : 1 (pid: 1329650)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002913-hsn0
  rank      : 18 (local_rank: 2)
  exitcode  : 1 (pid: 1329651)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid002913-hsn0
  rank      : 19 (local_rank: 3)
  exitcode  : 1 (pid: 1329652)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002913-hsn0
  rank      : 16 (local_rank: 0)
  exitcode  : 1 (pid: 1329649)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0207 17:32:12.446000 140350107757440 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 518584) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
W0207 17:32:12.455000 140569550056320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2182647 closing signal SIGTERM
W0207 17:32:12.455000 140569550056320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2182648 closing signal SIGTERM
W0207 17:32:12.455000 140569550056320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2182650 closing signal SIGTERM
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return launch_agent(self._config, self._entrypoint, list(args))
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid003089-hsn0
  rank      : 57 (local_rank: 1)
  exitcode  : 1 (pid: 518585)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid003089-hsn0
  rank      : 58 (local_rank: 2)
  exitcode  : 1 (pid: 518586)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid003089-hsn0
  rank      : 59 (local_rank: 3)
  exitcode  : 1 (pid: 518587)
  error_file: <N/A>
    run(args)
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid003089-hsn0
  rank      : 56 (local_rank: 0)
  exitcode  : 1 (pid: 518584)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002956-hsn0
  rank      : 26 (local_rank: 2)
  exitcode  : 1 (pid: 482839)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0207 17:32:12.478000 140569550056320 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2182649) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002976-hsn0
  rank      : 34 (local_rank: 2)
  exitcode  : 1 (pid: 2182649)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0207 17:32:12.503000 139972049111936 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 397534) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002900-hsn0
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 397535)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002900-hsn0
  rank      : 10 (local_rank: 2)
  exitcode  : 1 (pid: 397536)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid002900-hsn0
  rank      : 11 (local_rank: 3)
  exitcode  : 1 (pid: 397537)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002900-hsn0
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 397534)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0207 17:32:12.525000 140133753011072 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1605093) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002836-hsn0
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1605094)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002836-hsn0
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1605095)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-02-07_17:32:12
  host      : nid002836-hsn0
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1605096)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002836-hsn0
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1605093)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0207 17:32:12.554000 140005768510336 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1900359 closing signal SIGTERM
E0207 17:32:12.719000 140005768510336 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1900360) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
W0207 17:32:12.743000 140005768510336 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid002964-hsn0_1900317_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
W0207 17:32:12.746000 140005768510336 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid002964-hsn0_1900317_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
W0207 17:32:12.748000 140005768510336 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid002964-hsn0_1900317_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid002964-hsn0
  rank      : 30 (local_rank: 2)
  exitcode  : 1 (pid: 1900361)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-02-07_17:32:12
  host      : nid002964-hsn0
  rank      : 31 (local_rank: 3)
  exitcode  : 1 (pid: 1900362)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid002964-hsn0
  rank      : 29 (local_rank: 1)
  exitcode  : 1 (pid: 1900360)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0207 17:32:12.885000 140072797277056 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2172814 closing signal SIGTERM
W0207 17:32:12.886000 140072797277056 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2172815 closing signal SIGTERM
E0207 17:32:12.918000 140072797277056 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2172812) of binary: /pscratch/sd/k/klhhhhh/envs/nemo/bin/python3.10
W0207 17:32:12.942000 140072797277056 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid003000-hsn0_2172769_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
W0207 17:32:12.945000 140072797277056 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid003000-hsn0_2172769_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
W0207 17:32:12.948000 140072797277056 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid003000-hsn0_2172769_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/global/homes/k/klhhhhh/NeMo-modular-training/examples/nlp/language_modeling/megatron_gpt_pretraining.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-02-07_17:32:12
  host      : nid003000-hsn0
  rank      : 41 (local_rank: 1)
  exitcode  : 1 (pid: 2172813)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-07_17:32:12
  host      : nid003000-hsn0
  rank      : 40 (local_rank: 0)
  exitcode  : 1 (pid: 2172812)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0207 17:32:13.057000 139802892917632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 692152 closing signal SIGTERM
W0207 17:32:13.057000 139802892917632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 692153 closing signal SIGTERM
W0207 17:32:13.058000 139802892917632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 692154 closing signal SIGTERM
W0207 17:32:13.058000 139802892917632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 692155 closing signal SIGTERM
W0207 17:32:13.254000 139802892917632 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid003056-hsn0_692110_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
W0207 17:32:13.260000 139802892917632 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1218] The node 'nid003056-hsn0_692110_0' has failed to shutdown the rendezvous 'gpt_350m' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 114, in _call_store
    return getattr(self._store, store_op)(*args, **kwargs)
torch.distributed.DistNetworkError: Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/pscratch/sd/k/klhhhhh/envs/nemo/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 867, in _invoke_run
    num_nodes_waiting = rdzv_handler.num_nodes_waiting()
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1189, in num_nodes_waiting
    self._state_holder.sync()
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 428, in sync
    get_response = self._backend.get_state()
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 74, in get_state
    base64_state: bytes = self._call_store("get", self._key)
  File "/pscratch/sd/k/klhhhhh/envs/nemo/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 116, in _call_store
    raise RendezvousConnectionError(
torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details.
srun: error: nid002981: task 9: Exited with exit code 1
srun: Terminating StepId=35554199.2
srun: error: nid003100: task 15: Exited with exit code 1
srun: error: nid002892: task 1: Exited with exit code 1
srun: error: nid002956: task 6: Exited with exit code 1
srun: error: nid002920: task 5: Exited with exit code 1
srun: error: nid003072: task 13: Exited with exit code 1
srun: error: nid002913: task 4: Exited with exit code 1
srun: error: nid002976: task 8: Exited with exit code 1
srun: error: nid003060: task 12: Exited with exit code 1
srun: error: nid002904: task 3: Exited with exit code 1
srun: error: nid003089: task 14: Exited with exit code 1
srun: error: nid003056: task 11: Exited with exit code 1
srun: error: nid002836: task 0: Exited with exit code 1
srun: error: nid002900: task 2: Exited with exit code 1
srun: error: nid002964: task 7: Exited with exit code 1
srun: error: nid003000: task 10: Exited with exit code 1
